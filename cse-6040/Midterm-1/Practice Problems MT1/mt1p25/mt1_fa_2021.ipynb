{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "# Midterm 1, Fall 2021: Chess Ratings #\n",
                "\n",
                "_Version 1.0_\n",
                "\n",
                "Change Log:\n",
                "1.0 - Initial Release\n",
                "\n",
                "This problem builds on your knowledge of **Python data structures, string processing, and implementing mathematical functions**.\n",
                "\n",
                "For other preliminaries and pointers, refer back to the Piazza post titled **\"Midterm 1 Release Notes\"**.\n",
                "- Total Exercises: **8**  \n",
                "- Total Points: **16**\n",
                "- Time Limit: **3 Hours**\n",
                "\n",
                "Each exercise builds logically on the previous one, but you may **solve them in any order**. That is, if you can't solve an exercise, you can still move on and try the next one. **However, if you see a code cell introduced by the phrase, \"Sample result for ...\", please run it.** Some demo cells in the notebook may depend on these precomputed results.\n",
                "\n",
                "The point values of individual exercises are as follows:\n",
                "\n",
                "- Exercise 0: 3 points\n",
                "- Exercise 1: 2 points\n",
                "- Exercise 2: 1 points\n",
                "- Exercise 3: 2 points\n",
                "- Exercise 4: 1 points\n",
                "- Exercise 5: 3 points\n",
                "- Exercise 6: 2 points\n",
                "- Exercise 7: 2 points\n",
                "\n",
                "\n",
                "**Good luck!**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "## Elo Ratings\n",
                "\n",
                "The Elo (rhymes with \"Hello\") rating system is a widely used method for quantifying relative skill levels of players in a game or sport. The method was originally used to rate chess players and is named for its creator, Arpad Elo. This system is very simple but is able to rate players much more effectively than a win/loss record.\n",
                "\n",
                "On a high level, the winning player in a game takes rating points away from the losing player. How many points change hands is determined by the difference in the initial ratings of each player. For example, if a highly rated player records a victory over a lower rated player, then they would gain only a few points. This is reflective of the highly rated player being expected to win. However, if the lower rated player is able to pull off an upset, a larger quantity of points would be exchanged. The idea is that over time the system will adjust players' ratings to their true relative skill levels. Additionally, the difference in Elo ratings between two players can be used to calculate the expectation for the number of wins each player would accrue, which is often expressed as \"win probability\". \n",
                "\n",
                "Here we will extract data from a recent chess tournament that captures players' ratings at the start of the tournament and the outcome of all games played. We will then use that data to calculate expected wins based on the matchups and compare our expectation with the observed results. Finally we will determine the updated Elo ratings for the players. There are many variations on this system, but here we will use the original version. You can find more information about the Elo rating system [here](https://en.wikipedia.org/wiki/Elo_rating_system)\n",
                "\n",
                "Let's get started by taking a look at the data!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n",
                "\n",
                "import run_tests as test_utils\n",
                "raw_data = test_utils.read_raw_data('Bucharest2021.pgn')\n",
                "test_utils.get_mem_usage_str()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "Take note of how the data is **split** into sections by **blank lines** (`'\\n\\n'`); this fact might be useful later on! _(hint! hint!)_ Here are the first 4 sections."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "demo_raw_data = '\\n\\n'.join(raw_data.split('\\n\\n')[:4])\n",
                "print(demo_raw_data)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "The sections in the raw data alternate between **metadata** and **moves data**. The metadata is information about the game, such as who is playing with what pieces, the ratings of each player, and the results of the game. The moves data contains a record of each chess move executed in the game. Since players' Elo ratings are only affected by the outcomes of the games, we are primarily concerned with the metadata."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "## Exercise 0 (3 points)\n",
                "\n",
                "The first thing we need to do in our analysis is get the data in a more structured form. \n",
                "\n",
                "Fill out the function `extract_games(raw_data)` in the code cell below with the following requirements:\n",
                "\n",
                "Given a string read from a text file `raw_data`, extract the following information about each game and store in a **list of dictionaries** `games`. Below are details for what one of these dictionaries should look like: \n",
                "* `games[i]['white_player']` - String - Name of the player assigned the white pieces.\n",
                "  * Example from `raw_data`: [White \"Deac,Bogdan-Daniel\"]\n",
                "  * Example value: `'Deac,Bogdan-Daniel'`  \n",
                "  * Value type: `str`  \n",
                "  \n",
                "  \n",
                "* `games[i]['black_player']` - String - Name of the player assigned the black pieces.\n",
                "  * Example from `raw_data`: [Black \"Giri,A\"]\n",
                "  * Example value: `'Giri,A'`  \n",
                "  * Value type: `str`\n",
                "    \n",
                "\n",
                "* `games[i]['white_rating']` - Integer - Pre-tournament rating of the white player.\n",
                "  * Example from `raw_data`: [WhiteElo \"2627\"]\n",
                "  * Example value: `2627`  \n",
                "  * Value type: `int`\n",
                "    \n",
                "    \n",
                "* `games[i]['black_rating']` - Integer - Pre-tournament rating of the black player.\n",
                "  * Example from `raw_data`: [BlackElo \"2780\"]\n",
                "  * Example value: `2780`  \n",
                "  * Value type: `int`\n",
                "    \n",
                "    \n",
                "* `games[i]['result']` - String - Result of the game.\n",
                "  * Example from `raw_data`: [Result \"1/2-1/2\"]\n",
                "  * Example value: `'1/2-1/2'`\n",
                "  * Value type: `str`\n",
                "\n",
                "You may assume that the required metadata is included, that sections are separated by blank lines, and that the sections alternate between metadata and moves data (starting with metadata). Additional metadata tags (beyond the 5 you are tasked with extracting) may be present, but they should be ignored. The ordering of the metadata **may be different** from the example above. Additionally, the moves data sections **may not be formatted** the same way as the example above.\n",
                "\n",
                "A demo of your function run on the `demo_raw_data` defined above is included in the solution cell. The result should be:\n",
                "```\n",
                "[{  'white_player': 'Deac,Bogdan-Daniel',\n",
                "    'black_player': 'Giri,A',\n",
                "    'result': '1/2-1/2',\n",
                "    'white_rating': 2627,\n",
                "    'black_rating': 2780},\n",
                "  { 'white_player': 'Lupulescu,C', \n",
                "    'black_player': 'Aronian,L', \n",
                "    'result': '1/2-1/2', \n",
                "    'white_rating': 2656, \n",
                "    'black_rating': 2781}]\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "To help you get started, consider the following snippet, which converts `demo_raw_data` into a nested list of lists. A similar strategy may be helpful in processing the `raw_data` parameter in the exercise."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "demo_metadata_list = [metadata.splitlines() for metadata in demo_raw_data.split('\\n\\n')[::2]]\n",
                "print(f'type(demo_metadata_list[0]): {type(demo_metadata_list[0])}') # outer list items are lists\n",
                "print(f'type(demo_metadata_list[0][0]): {type(demo_metadata_list[0][0])}') # inner list items are strings\n",
                "demo_metadata_list"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_games(raw_data):\n",
                "    import re\n",
                "    dlist = []\n",
                "    demo_metadata_list = [metadata.splitlines() for metadata in raw_data.split('\\n\\n')]\n",
                "    key_map = {'White': 'white_player', 'Black': 'black_player', 'Result': 'result', 'WhiteElo': 'white_rating', 'BlackElo': 'black_rating'}\n",
                "    \n",
                "    for l in demo_metadata_list:\n",
                "        idict = {}\n",
                "        for e in l:\n",
                "            # Searching for the key and value in each entry\n",
                "            am = re.search(r'\\s(.*)', e)\n",
                "            bm = re.search(r'^(.*?)\\s', e)\n",
                "            \n",
                "            if am and bm:\n",
                "                amw = am.group(1).replace(']', '')\n",
                "                bmw = bm.group(1).replace('[', '')\n",
                "                idict[bmw] = amw.strip('\"')\n",
                "        \n",
                "        # Creating new_dict only with keys that are in key_map and present in idict\n",
                "        new_dict = {key_map[key]: idict[key] for key in idict if key in key_map}\n",
                "        \n",
                "        if new_dict:\n",
                "            # Check if 'white_rating' and 'black_rating' exist before converting\n",
                "            if 'white_rating' in new_dict:\n",
                "                new_dict['white_rating'] = int(new_dict['white_rating'])\n",
                "            if 'black_rating' in new_dict:\n",
                "                new_dict['black_rating'] = int(new_dict['black_rating'])\n",
                "            dlist.append(new_dict)\n",
                "            \n",
                "    return dlist\n",
                "# extract_games(demo_raw_data)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "The test cell below runs your function **many times**. Remove or comment out any `print` statements to avoid generating excessive output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "ex0_test",
                    "locked": true,
                    "points": "3",
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "# `ex0_test`: Test cell\n",
                "from run_tests import ex0_test\n",
                "for _ in range(100):\n",
                "    ex0_test(10, 4, extract_games)\n",
                "print('Passed!')\n",
                "\n",
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n",
                "test_utils.get_mem_usage_str()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "**Run the following cell, even if you skipped Exercise 0.**\n",
                "\n",
                "We are loading a pre-computed solution that will be used in the following sections. The first two sections items in the list are displayed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "# Sample result for ex0\n",
                "games_metadata = test_utils.read_pickle('games_metadata')\n",
                "print(games_metadata[:2])\n",
                "test_utils.get_mem_usage_str()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "## Exercise 1 (2 points)\n",
                "\n",
                "The next bit of information we will need in our analysis is the outcome of each player's games paired with their opponent.\n",
                "\n",
                "Fill out the function `extract_player_results(games)` in the code cell below with the following requirements:\n",
                "\n",
                "Given `games`, a list of dictionaries containing the metadata for each game, create dictionary `player_results` mapping each player's name to a list of the outcomes of that player's games. Each outcome should include the opponent's name (String) and the number of points that the player received (Float) as the outcome of the game as a Tuple. \n",
                "\n",
                "The order of tuples in the list associated with each player should be the **same as the order of the matchups in `games`**. \n",
                "\n",
                "You should interpret the value associated with `'result'` as `\"<white player points>-<black player points>\"` separated by a dash \"-\". There are three possible outcomes of a game of chess: White wins (`'1-0'`), black wins (`'0-1'`), or draw (`'1/2-1/2'`).\n",
                "\n",
                "For example, if the input is:\n",
                "\n",
                "`[{'white_player': 'Dwight Schrute', 'black_player: 'Jim Halpert', 'result': '1-0'}, {'white_player': 'Stanley Hudson', 'black_player': 'Dwight Schrute', 'result': '1/2-1/2'}]`\n",
                "\n",
                "Then the output should be:\n",
                "\n",
                "`{'Dwight Schrute': [('Jim Halpert', 1.0), ('Stanley Hudson', 0.5)], 'Jim Halpert': [('Dwight Schrute', 0.0)], 'Stanley Hudson': [('Dwight Schrute', 0.5)]}`\n",
                "\n",
                "You can assume that each dictionary in `games` will have the keys `'white_player'`, `'black_player'`, and `'result'` and that the values associated with each of those keys are Strings. There may be duplicated matchups where the same two players are paired in the tournament more than once. These cases should be handled the same as any other game and do not require any special treatment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "collapsed": true,
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "demo_games_metadata = [{'white_player': 'Dwight Schrute', 'black_player': 'Jim Halpert', 'result': '1-0'}, {'white_player': 'Stanley Hudson', 'black_player': 'Dwight Schrute', 'result': '1/2-1/2'}]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": false,
                    "solution": false
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'Dwight Schrute': [('Jim Halpert', 1.0), ('Stanley Hudson', 0.5)],\n",
                            " 'Jim Halpert': [('Dwight Schrute', 0.0)],\n",
                            " 'Stanley Hudson': [('Dwight Schrute', 0.5)]}"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "def extract_player_results(games):\n",
                "    player_results = {}\n",
                "    \n",
                "    # Function to convert result string to numeric value for player perspective\n",
                "    def result_to_numeric(result, player_is_white):\n",
                "        if result == '1-0':\n",
                "            return 1.0 if player_is_white else 0.0\n",
                "        elif result == '0-1':\n",
                "            return 0.0 if player_is_white else 1.0\n",
                "        elif result == '1/2-1/2':\n",
                "            return 0.5\n",
                "        else:\n",
                "            return None  # Handle unexpected result format\n",
                "    \n",
                "    for game in games:\n",
                "        # Process for white player\n",
                "        wp = game['white_player']\n",
                "        bp = game['black_player']\n",
                "        result = game['result']\n",
                "        \n",
                "        if wp not in player_results:\n",
                "            player_results[wp] = []\n",
                "        player_results[wp].append((bp, result_to_numeric(result, True)))\n",
                "        \n",
                "        # Process for black player\n",
                "        if bp not in player_results:\n",
                "            player_results[bp] = []\n",
                "        player_results[bp].append((wp, result_to_numeric(result, False)))\n",
                "    \n",
                "    return player_results\n",
                "\n",
                "# Demo\n",
                "extract_player_results(demo_games_metadata)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "The test cell below runs your function **many times**. Remove or comment out any `print` statements to avoid generating excessive output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "ex1_test",
                    "locked": true,
                    "points": "2",
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "# `ex1_test`: Test cell\n",
                "from run_tests import ex1_test\n",
                "for _ in range(100):\n",
                "    ex1_test(10, 4, extract_player_results)\n",
                "print('Passed!')\n",
                "\n",
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n",
                "test_utils.get_mem_usage_str()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "**Run the following cell, even if you skipped Exercise 1.**\n",
                "\n",
                "We are loading a pre-computed solution that will be used in the following sections. The first two entries are displayed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "# Sample result for ex1\n",
                "player_results = test_utils.read_pickle('player_results')\n",
                "{k:v for k, v in list(player_results.items())[:2]}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "## Exercise 2 (1 point)\n",
                "\n",
                "Our next task is to compute the total tournament score for each player.\n",
                "\n",
                "Fill in the function `calculate_score(player_results)` satisfying the following requirements:\n",
                "\n",
                "Given a dictionary `player_results` mapping player names to their tournament results (similar to the output of Excercise 1), create a **new** dictionary `player_scores` that maps each player (String) to their total score for the tournament (Float).\n",
                "\n",
                "For example, given the following input: \n",
                "\n",
                "`{'Angela Martin': [('Oscar Martinez', 1.0), ('Kevin Malone', 0.5), ('Andy Bernard', 0.0)], 'Michael Scott': [('Pam Halpert', 0.0), ('Toby Flenderson', 0.0), ('Todd Packer', 0.0)]}`\n",
                "\n",
                "Your function should output:\n",
                "\n",
                "`{'Angela Martin': 1.5, 'Michael Scott': 0.0}`\n",
                "\n",
                "(Michael isn't exactly a chess prodigy...)\n",
                "\n",
                "You can assume that the lists keyed to each String in the input will be of the form (String, Float). You do not need to worry about verifying that all of the games implied by the input are present. If you look closely at the example, you will see that this is **not** the case.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "collapsed": true,
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "\n",
                "demo_player_results = {'Angela Martin': [('Oscar Martinez', 1.0), ('Kevin Malone', 0.5), ('Andy Bernard', 0.0)], 'Michael Scott': [('Pam Halpert', 0.0), ('Toby Flenderson', 0.0), ('Todd Packer', 0.0)]}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'Angela Martin': 1.5, 'Michael Scott': 0.0}\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "{'Angela Martin': 1.5, 'Michael Scott': 0.0}"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "def calculate_score(player_results):\n",
                "    player_scores = {}\n",
                "    for k,v in player_results.items():\n",
                "        player = k\n",
                "#         print(k)\n",
                "        r = 0\n",
                "        for  result in player_results[k]:\n",
                "            r = r+result[1]\n",
                "        player_scores[k] = r\n",
                "    print(player_scores)\n",
                "    return player_scores    \n",
                "\n",
                "# Demo\n",
                "calculate_score(demo_player_results)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "The test cell below runs your function **many times**. Remove or comment out any `print` statements to avoid generating excessive output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "ex2_test",
                    "locked": true,
                    "points": "1",
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "# `ex2_test`: Test cell\n",
                "from run_tests import ex2_test\n",
                "for _ in range(200):\n",
                "    ex2_test(10, 4, calculate_score)\n",
                "print('Passed!')\n",
                "\n",
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n",
                "test_utils.get_mem_usage_str()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "**Run the following cell, even if you skipped Exercise 2.**\n",
                "\n",
                "We are loading a pre-computed solution that will be used in the following sections. The first two entries are displayed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "# Sample result for ex2\n",
                "player_scores = test_utils.read_pickle('player_scores')\n",
                "{k:v for k, v in list(player_scores.items())[:2]}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "## Exercise 3 (2 points)\n",
                "\n",
                "Our next task is to extract the Elo rating of each player from the metadata.\n",
                "\n",
                "Fill in the function `extract_ratings(games)` to satisfy the following requirements:\n",
                "\n",
                "Given a list of dictionaries, `games`, create a dictionary `player_ratings` that maps each player to their Elo rating before the tournament. You can assume that each dictionary in `games` will have the following keys and value types: `'white_player'`: (String), `'black_player'`: (String), `'white_rating'`: (Integer), and `'black_rating'`: (Integer).\n",
                "\n",
                "Additionally, if the same player has different ratings in the input, your function should raise a `ValueError`.\n",
                "\n",
                "For example:\n",
                "\n",
                "Input : `[{'white_player': 'Jim Halpert', 'black_player': 'Darryl Philbin', 'white_rating': 1600, 'black_rating': 1800}, {'white_player': 'Darryl Philbin', 'black_player': 'Phyllis Vance', 'white_rating': 1800, 'black_rating': 1700}]`\n",
                "\n",
                "Output : `{'Darryl Philbin': 1800, 'Jim Halpert': 1600, 'Phyllis Vance': 1700}`\n",
                "\n",
                "Input : `[{'white_player': 'Jim Halpert', 'black_player': 'Darryl Philbin', 'white_rating': 1600, 'black_rating': 1800}, {'white_player': 'Darryl Philbin', 'black_player': 'Phyllis Vance', 'white_rating': 1850, 'black_rating': 1700}]`\n",
                "\n",
                "Here `'Darryl Philbin'` has two ratings: 1800 in his first game and 1850 in his second. Your function should raise a `ValueError`!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "collapsed": true,
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "demo_metadata_good = [{'white_player': 'Jim Halpert', 'black_player': 'Darryl Philbin', 'white_rating': 1600, 'black_rating': 1800}, {'white_player': 'Darryl Philbin', 'black_player': 'Phyllis Vance', 'white_rating': 1800, 'black_rating': 1700}]\n",
                "demo_metadata_bad = [{'white_player': 'Jim Halpert', 'black_player': 'Darryl Philbin', 'white_rating': 1600, 'black_rating': 1800}, {'white_player': 'Darryl Philbin', 'black_player': 'Phyllis Vance', 'white_rating': 1850, 'black_rating': 1700}]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Correctly raised ValueError\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "{'Jim Halpert': 1600, 'Darryl Philbin': 1800, 'Phyllis Vance': 1700}"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "def extract_ratings(games):\n",
                "\n",
                "    player_ratings = {}\n",
                "    \n",
                "    for game in games:\n",
                "        # Extract player names and their ratings from the game\n",
                "        white_player, black_player = game['white_player'], game['black_player']\n",
                "        white_rating, black_rating = game['white_rating'], game['black_rating']\n",
                "        \n",
                "        # Check and update white player's rating\n",
                "        if white_player in player_ratings:\n",
                "            if player_ratings[white_player] != white_rating:\n",
                "                raise ValueError(f\"{white_player} has inconsistent ratings.\")\n",
                "        else:\n",
                "            player_ratings[white_player] = white_rating\n",
                "        \n",
                "        # Check and update black player's rating\n",
                "        if black_player in player_ratings:\n",
                "            if player_ratings[black_player] != black_rating:\n",
                "                raise ValueError(f\"{black_player} has inconsistent ratings.\")\n",
                "        else:\n",
                "            player_ratings[black_player] = black_rating\n",
                "    \n",
                "    return player_ratings\n",
                "\n",
                "# Demo\n",
                "try:\n",
                "    extract_ratings(demo_metadata_bad)\n",
                "    print('This should raise a ValueError')\n",
                "except ValueError:\n",
                "    print('Correctly raised ValueError')\n",
                "extract_ratings(demo_metadata_good)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "The test cell below runs your function **many times**. Remove or comment out any `print` statements to avoid generating excessive output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "ex3_test",
                    "locked": true,
                    "points": "2",
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "# `ex3_test`: Test cell\n",
                "from run_tests import ex3_test\n",
                "for _ in range(200):\n",
                "    ex3_test(10, 4, extract_ratings)\n",
                "print('Passed!')\n",
                "\n",
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "**Run the following cell, even if you skipped Exercise 3.**\n",
                "\n",
                "We are loading a pre-computed solution that will be used in the following sections. The first two entries are displayed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "# Sample result for ex3\n",
                "player_ratings = test_utils.read_pickle('player_ratings')\n",
                "{k:v for k, v in list(player_ratings.items())[:2]}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "## Exercise 4 (1 point)\n",
                "\n",
                "The last task before we begin analysis is to implement some functionality to calculate the expected result of a match based on the Elo ratings of each player.\n",
                "\n",
                "Fill out the function `expected_match_score(r_player, r_opponent)` to satisfy the following requirements:\n",
                "\n",
                "Given a player's rating (Integer) and their opponent's rating (Integer), compute the player's expected score in a game against that opponent. The formula for the expected score is:\n",
                "\n",
                "$$\\text{Expected Score} =  \\frac{1}{1 + 10^{d}}$$\n",
                "where \n",
                "$$d = \\frac{r_{\\text{opponent}} - r_{\\text{player}}}{400}$$\n",
                "\n",
                "Output the expected score as a Float. **Do not round**.\n",
                "\n",
                "For example:\n",
                "\n",
                "`expected_match_score(1900, 1500)` should return about `0.909`  \n",
                "`expected_match_score(1500, 1500)` should return about `0.5`  \n",
                "`expected_match_score(1900, 1700)` should return about `0.76`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "collapsed": true,
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "demo_ratings = [(1900, 1500), (1500, 1500), (1900, 1700)]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "expected_match_score(1900, 1500) = 0.9090909090909091\n",
                        "expected_match_score(1500, 1500) = 0.5\n",
                        "expected_match_score(1900, 1700) = 0.7597469266479578\n"
                    ]
                }
            ],
            "source": [
                "def expected_match_score(r_player, r_opponent):\n",
                "    d = (r_opponent - r_player)/400\n",
                "    es = 1/(1+(10)**d)\n",
                "    return es\n",
                "\n",
                "# Demo\n",
                "for rp, ro in demo_ratings:\n",
                "    print(f'expected_match_score({rp}, {ro}) = {expected_match_score(rp, ro)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "The test cell below runs your function **many times**. Remove or comment out any `print` statements to avoid generating excessive output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "ex4_test",
                    "locked": true,
                    "points": "1",
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "# `ex4_test`: Test cell\n",
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n",
                "from run_tests import ex4_test\n",
                "for _ in range(200):\n",
                "    ex4_test(expected_match_score)\n",
                "print('Passed!')\n",
                "\n",
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n",
                "test_utils.get_mem_usage_str()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "## Aside - Functional Programming\n",
                "\n",
                "It is often useful to write functions which take other functions as arguments. Inside of your function, the functional argument is called in a consistent way. This allows the caller of your function to customize it's behavior. \n",
                "\n",
                "Here is an over-engineered arithmetic calculator as an example. These functions define mathematical operations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "# add\n",
                "def a(a, b):\n",
                "    return a+b\n",
                "# subtract\n",
                "def s(a, b):\n",
                "    return a-b\n",
                "# multiply\n",
                "def m(a, b):\n",
                "    return a*b\n",
                "# divide\n",
                "def d(a,b):\n",
                "    return a/b"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "This function, `calc`, takes the two numbers as an argument and a third argument which determines how they are combined."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "def calc(a, b, opp):\n",
                "    return opp(a,b)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "Now we can use any function that takes two arguments, like the 4 defined above to determine the behavior of `calc`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "calc(3,5,a)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "calc(3,5,d)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "## Exercise 5 (3 points)\n",
                "\n",
                "Our next task is to write some functionality to determine each player's expected tournament score.\n",
                "\n",
                "Fill in the function `expected_tournament_score(player_results, player_ratings, es_func)` to satisfy the following requirements:\n",
                "\n",
                "Given a dictionary, `player_results`, mapping players to their tournament results as a list of tuples (similar to the output from Exercise 1) and a dictionary, `player_ratings`, mapping players to their Elo ratings, compute the **total** expected score for each player (you only need to compute total expected score for players that are keys in `player_results`). The total expected score is simply the sum of the expected scores for each of that players games. Output the results as a dictionary mapping players (String) to their expected tournament score (Float).\n",
                "\n",
                "The third argument `es_func` is a function that takes two arguments (the player's rating and opponent's rating respectively) and returns an \"expected score\". You should use it to compute the expected scores for this exercise. **It might not be the same as the solution to Exercise 4!**\n",
                "\n",
                "A call to `es_func(1450, 1575)` inside of your function would compute the \"expected score\" for the 1450-rated player against a 1575-rated player.\n",
                "\n",
                "For example given:\n",
                "\n",
                "`player_results = {'Angela Martin': [('Dwight Schrute', 1.0), ('Stanley Hudson', 0.5)], 'Dwight Schrute': [('Angela Martin', 0.0), ('Jim Halpert', 0.5)]}`\n",
                "\n",
                "`player_ratings = {'Angela Martin': 1600, 'Dwight Schrute': 1750, 'Stanley Hudson': 1800, 'Jim Halpert': 1700}`\n",
                "\n",
                "`es_func = lambda r_player, r_opponent: float(r_player - r_opponent)`\n",
                "\n",
                "The output would be:\n",
                "\n",
                "`{'Angela Martin': -350.0, 'Dwight Schrute': 200.0}`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "collapsed": true,
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "demo_player_results = {'Angela Martin': [('Dwight Schrute', 1.0), ('Stanley Hudson', 0.5)], 'Dwight Schrute': [('Angela Martin', 0.0), ('Jim Halpert', 0.5)]}\n",
                "demo_player_ratings = {'Angela Martin': 1600, 'Dwight Schrute': 1750, 'Stanley Hudson': 1800, 'Jim Halpert': 1700}\n",
                "demo_es_func = lambda r_player, r_opponent: float(r_player - r_opponent)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "defaultdict(float, {'Angela Martin': -350.0, 'Dwight Schrute': 200.0})"
                        ]
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "def expected_tournament_score(player_results, player_ratings, es_func):\n",
                "     from collections import defaultdict\n",
                "     expected = defaultdict(float)\n",
                "     for player, results in player_results.items():\n",
                "         for opp, _ in results:\n",
                "             p_rating = player_ratings[player]\n",
                "             opp_rating = player_ratings[opp]\n",
                "             expected[player] += es_func(p_rating, opp_rating)\n",
                "     return expected\n",
                "\n",
                "# Demo\n",
                "expected_tournament_score(demo_player_results, demo_player_ratings, demo_es_func) "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "The test cell below runs your function **many times**. Remove or comment out any `print` statements to avoid generating excessive output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "ex5_test",
                    "locked": true,
                    "points": "3",
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "# `ex5_test`: Test cell\n",
                "from run_tests import ex5_test\n",
                "for _ in range(200):\n",
                "    ex5_test(10, 4, expected_tournament_score)\n",
                "print('Passed!')\n",
                "\n",
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n",
                "test_utils.get_mem_usage_str()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "**Run the following cell, even if you skipped Exercise 5.**\n",
                "\n",
                "We are loading a pre-computed solution that will be used in the following sections. The first two entries are displayed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "# Sample result for ex5\n",
                "player_expected_score = test_utils.read_pickle('player_expected_score')\n",
                "{k:v for k, v in list(player_expected_score.items())[:2]}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "## Exercise 6 (2 points)\n",
                "\n",
                "Fill in the function `compute_final_ratings(player_scores, expected_player_scores, player_ratings)` to meet the following requirements:\n",
                "\n",
                "Given three dictionaries:\n",
                "\n",
                "* `player_scores`: mapping players (String) to their observed tournament scores (Float)  \n",
                "* `expected_player_scores`: mapping players (String) to their expected tournament scores (Float)  \n",
                "* `player_ratings`: mapping players (String) to their pre-tournament Elo ratings (Float)  \n",
                "\n",
                "calculate each player's post-tournament Elo ratings using this formula:\n",
                "\n",
                "$$\\text{Rating}_{\\text{post}} = \\text{Rating}_{\\text{pre}} + 10(\\text{Score}_{\\text{observed}} - \\text{Score}_{\\text{expected}})$$\n",
                "\n",
                "Return a dictionary mapping each player (String) to their post-tournament rating **rounded to the nearest integer**.\n",
                "\n",
                "You can assume that all keys are common between the three input dictionaries.\n",
                "\n",
                "For example:\n",
                "\n",
                "`player_scores = {'Jim Halpert': 3.0, 'Dwight Schrute': 4.0, 'Stanley Hudson': 3.0}`\n",
                "\n",
                "`expected_player_scores = {'Jim Halpert': 2.736, 'Dwight Schrute': 4.67, 'Stanley Hudson': 2.85}`\n",
                "\n",
                "`player_ratings = {'Jim Halpert': 1500, 'Dwight Schrute': 1575, 'Stanley Hudson': 1452}`\n",
                "\n",
                "Results:\n",
                "`{'Jim Halpert': 1503, 'Dwight Schrute': 1568, 'Stanley Hudson': 1454}`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "collapsed": true,
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "demo_player_scores = {'Jim Halpert': 3.0, 'Dwight Schrute': 4.0, 'Stanley Hudson': 3.0}\n",
                "demo_expected_player_scores = {'Jim Halpert': 2.736, 'Dwight Schrute': 4.67, 'Stanley Hudson': 2.85}\n",
                "demo_player_ratings = {'Jim Halpert': 1500, 'Dwight Schrute': 1575, 'Stanley Hudson': 1452}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'Jim Halpert': 1503, 'Dwight Schrute': 1568, 'Stanley Hudson': 1454}"
                        ]
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "def compute_final_ratings(player_scores, expected_player_scores, player_ratings):\n",
                "    post = {}\n",
                "    players = player_scores.keys()\n",
                "    for player in players:\n",
                "        oscore = player_scores[player]\n",
                "        escore = expected_player_scores[player]\n",
                "        rating = player_ratings[player]\n",
                "        post_rating = round(rating + 10*(oscore - escore))\n",
                "        post[player] = post_rating\n",
                "    return post\n",
                "\n",
                "# Demo\n",
                "compute_final_ratings(demo_player_scores, demo_expected_player_scores, demo_player_ratings)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "The test cell below runs your function **many times**. Remove or comment out any `print` statements to avoid generating excessive output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "ex6_test",
                    "locked": true,
                    "points": "2",
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "# `ex6_test`: Test cell\n",
                "from run_tests import ex6_test\n",
                "for _ in range(200):\n",
                "    ex6_test(10, compute_final_ratings)\n",
                "print('Passed!')\n",
                "\n",
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n",
                "test_utils.get_mem_usage_str()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "**Run the following cell, even if you skipped Exercise 6.**\n",
                "\n",
                "We are loading a pre-computed solution that will be used in the following sections. The first two entries are displayed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "# Sample result for ex6\n",
                "player_final_ratings = test_utils.read_pickle('player_final_ratings')\n",
                "{k:v for k, v in list(player_final_ratings.items())[:2]}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "## Exercise 7 (2 points)\n",
                "\n",
                "The last task we have is to compute the change in rating. This isn't just an intermediate step in Exercise 6, because we have to handle some special cases as well.\n",
                "\n",
                "Fill in the function `compute_deltas(old_ratings, new_ratings)` to meet the following requirements:\n",
                "\n",
                "Given dictionaries `old_ratings` mapping players (String) to their pre-tournament Elo ratings (Integer) and `new_ratings` mapping players (String) to their post-tournament Elo ratings, determine the change in each player's rating. Return your result as a dictionary mapping players (String) to their delta (Integer).\n",
                "\n",
                "Compute the delta as $$\\Delta = \\text{Rating}_{\\text{new}} - \\text{Rating}_{\\text{old}}$$\n",
                "\n",
                "If a player is not present as a key in the `old_ratings` input but is present as a key in the `new_ratings` input, then assume this is a new player with a starting rating of `1200`. Likewise, if a player is present as a key in `old_ratings` but is not present in `new_ratings`, assume that player did not play in the tournament and their rating is unchanged.\n",
                "\n",
                "For example:\n",
                "\n",
                "`old_ratings = {'Ryan Howard': 1755, 'Dwight Schrute': 1675}`\n",
                "\n",
                "`new_ratings = {'Michael Scott': 1250, 'Ryan Howard': 1750}`\n",
                "\n",
                "Should return:\n",
                "\n",
                "`{'Michael Scott': 50, 'Ryan Howard': -5, 'Dwight Schrute': 0}`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {
                "collapsed": true,
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "demo_old_ratings = {'Ryan Howard': 1755, 'Dwight Schrute': 1675}\n",
                "demo_new_ratings = {'Michael Scott': 1250, 'Ryan Howard': 1750}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'Ryan Howard': -5, 'Michael Scott': 50, 'Dwight Schrute': 0}"
                        ]
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "def compute_deltas(old_ratings, new_ratings):\n",
                "     both = set(old_ratings.keys()) & set(new_ratings.keys())\n",
                "     old = set(old_ratings.keys()) - set(new_ratings.keys())\n",
                "     new = set(new_ratings.keys()) - set(old_ratings.keys())\n",
                "     deltas = {}\n",
                "     for player in both:\n",
                "        deltas[player] = new_ratings[player] - old_ratings[player]\n",
                "     for player in new:\n",
                "        deltas[player] = new_ratings[player] - 1200\n",
                "     for player in old:\n",
                "         deltas[player] = 0\n",
                "     return deltas \n",
                "    \n",
                "\n",
                "# Demo\n",
                "compute_deltas(demo_old_ratings, demo_new_ratings)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "The test cell below runs your function **many times**. Remove or comment out any `print` statements to avoid generating excessive output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "ex7_test",
                    "locked": true,
                    "points": "2",
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "# `ex7_test`: Test cell\n",
                "from run_tests import ex7_test\n",
                "for _ in range(200):\n",
                "    ex7_test(10, compute_deltas)\n",
                "print('Passed!')\n",
                "\n",
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n",
                "test_utils.get_mem_usage_str()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "## Wrapping up\n",
                "After parsing all of the information from the text file, we can display a summary of the tournament results. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "df = pd.DataFrame(index=player_scores.keys())\n",
                "df['Initial Rating'] = pd.Series(player_ratings)\n",
                "df['Score'] = pd.Series(player_scores)\n",
                "df['Expected Score'] = pd.Series(player_expected_score)\n",
                "df['Final Rating'] = pd.Series(player_final_ratings)\n",
                "df['Delta'] = pd.Series(test_utils.read_pickle('player_deltas'))\n",
                "display(df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                }
            },
            "source": [
                "**Fin!** You’ve reached the end of this part. Don’t forget to restart and run all cells again to make sure it’s all working when run in sequence; and make sure your work passes the submission process. Good luck!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
